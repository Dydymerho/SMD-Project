services:
  # 1. Backend API
  ai-service:
    image: dinhtrng/smd-ai:latest
    ports:
      - "8000:8000"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3 # <--- Đã đổi lại thành llama3
    depends_on:
      - redis
      - ollama

  # 2. Worker
  worker:
    image: dinhtrng/smd-ai:latest
    command: celery -A app.worker.celery_app worker --loglevel=info -P solo
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3 # <--- Đã đổi lại thành llama3
    depends_on:
      - redis
      - ollama

  # 3. Redis
  redis:
    image: redis:alpine
    ports:
      - "6380:6379"

  # 4. Ollama
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_container
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  ollama_data:
