import os
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from app.schemas.ai_schema import CloPloCheckResponse

# Cáº¥u hÃ¬nh cá»©ng (Hardcode) cho nhanh, sau nÃ y Ä‘Æ°a vÃ o config sau
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3"

class LLMService:
    def __init__(self):
        # --- Sá»¬A ÄOáº N NÃ€Y ---
        # Æ¯u tiÃªn láº¥y tá»« Docker gá»­i vÃ o. Náº¿u khÃ´ng cÃ³ thÃ¬ má»›i dÃ¹ng localhost
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        # --------------------
        
        print(f"ğŸ¤– Äang káº¿t ná»‘i AI táº¡i: {base_url}") # In ra Ä‘á»ƒ debug xem nÃ³ trá» Ä‘i Ä‘Ã¢u

        self.llm = OllamaLLM(
            base_url=base_url,
            model="llama3", # Hoáº·c model báº¡n Ä‘ang dÃ¹ng
            temperature=0.2
        )

    async def generate_summary(self, text: str) -> str:
        template = """
        Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»c thuáº­t chuyÃªn nghiá»‡p.
        HÃ£y tÃ³m táº¯t ná»™i dung Ä‘á» cÆ°Æ¡ng mÃ´n há»c sau Ä‘Ã¢y má»™t cÃ¡ch ngáº¯n gá»n, sÃºc tÃ­ch báº±ng Tiáº¿ng Viá»‡t.
        
        Ná»™i dung: {text}
        
        TÃ³m táº¯t (Tiáº¿ng Viá»‡t):
        """
        prompt = PromptTemplate(template=template, input_variables=["text"])
        chain = prompt | self.llm
        result = await chain.ainvoke({"text": text})
        return result

    async def check_clo_plo_alignment(self, clo: str, plo: str) -> dict:
        parser = JsonOutputParser(pydantic_object=CloPloCheckResponse)
        template = """
        Báº¡n lÃ  chuyÃªn gia kiá»ƒm Ä‘á»‹nh giÃ¡o dá»¥c. ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ phÃ¹ há»£p giá»¯a CLO vÃ  PLO.
        CLO: {clo}
        PLO: {plo}
        Tráº£ vá» JSON: score (0-100), is_aligned (bool), reasoning (Tiáº¿ng Viá»‡t).
        {format_instructions}
        """
        prompt = PromptTemplate(
            template=template,
            input_variables=["clo", "plo"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        chain = prompt | self.llm | parser
        try:
            return await chain.ainvoke({"clo": clo, "plo": plo})
        except Exception as e:
            return {"score": 0, "is_aligned": False, "reasoning": f"Lá»—i: {str(e)}"}

    async def analyze_changes(self, old_text: str, new_text: str) -> str:
        # PROMPT Gáº®T Äá»‚ Ã‰P TIáº¾NG VIá»†T
        template = """
        Báº¡n lÃ  trá»£ lÃ½ áº£o ngÆ°á»i Viá»‡t Nam.
        Nhiá»‡m vá»¥: So sÃ¡nh hai vÄƒn báº£n.
        YÃŠU Cáº¦U: TUYá»†T Äá»I CHá»ˆ TRáº¢ Lá»œI Báº°NG TIáº¾NG VIá»†T, KHÃ”NG CHÃ€O Há»I, ÄI THáº²NG VÃ€O Váº¤N Äá»€.

        --- Báº¢N CÅ¨ ---
        {old_text}
        
        --- Báº¢N Má»šI ---
        {new_text}
        
        Káº¿t quáº£ phÃ¢n tÃ­ch thay Ä‘á»•i:
        """
        prompt = PromptTemplate(template=template, input_variables=["old_text", "new_text"])
        chain = prompt | self.llm
        result = await chain.ainvoke({"old_text": old_text, "new_text": new_text})
        return result

llm_service = LLMService()